{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "checksum": "0e2c3087183c4c8d86c8326ca3b000c5",
     "grade": false,
     "grade_id": "header",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Exam Simulation\n",
    "\n",
    "### General Instructions:\n",
    "\n",
    "This test is composed of **2** different parts, each one addressing a specific topic and accounting for the same amount of points (**16**). In particular, **Part 1** contains general coding questions, **Part 2** is dedicated specifically to data science.<br />\n",
    "\n",
    "Each part is in turn made of a set of exercises, with each exercise asking you either to answer some questions or to implement a certain function (possibly, you will need to implement your own \"helper\" functions as well, if doing so makes the overall task simpler to achieve). \n",
    "Plus, each exercise accounts for a certain number of points, which you will earn **if and only if** the answer you provide is correct or the implementation you come up with passes successfully **all** the tests (both those that are visible to you and those that are hidden).<br />\n",
    "\n",
    "To actually write down your implementation, make sure to fill in any place that says <code style=\"color:green\">**_# YOUR CODE HERE_**</code>.<br />\n",
    "Once you are done, save this notebook and rename it by replacing <code>**YOUR_USERNAME_HERE**</code> suffix with the your actual username. To be consistent, we are expecting your username to be composed by your first name's initial, followed by your full lastname. As an example, in my case <code>**exam-simulation-YOUR_USERNAME_HERE.ipynb**</code> must be saved as <code>**exam-simulation-gmdinunzio.ipynb**</code>.<br />\n",
    "\n",
    "Finally, go to our [Moodle page](https://elearning.unipd.it/math/course/view.php?id=243) and check for the \"_Exam Simulation_\" item; there, you will be able to upload your notebook file to be graded.\n",
    "\n",
    "The archive you have downloaded (<code style=\"color:magenta\">**exam-simulation.zip**</code>) is organized according to the following directory structure:\n",
    "\n",
    "--<code style=\"color:red\">**exam-simulation**</code> (root)<br />\n",
    "|----<code style=\"color:green\">**exam-simulation-YOUR_USERNAME_HERE.ipynb**</code> (_this_ notebook)<br />\n",
    "|----<code style=\"color:blue\">**part-1**</code><br />\n",
    "|----|----<code>**amazon_reviews.txt**</code><br />\n",
    "|----<code style=\"color:blue\">**part-2**</code><br />\n",
    "|----|----<code>**dataset.csv**</code><br />\n",
    "|----|----<code>**README.txt**</code><br />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "checksum": "10903b7e2a963c65c49b8c2b28aac02d",
     "grade": false,
     "grade_id": "import",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "# Adding the following line, allows Jupyter Notebook to visualize plots\n",
    "# produced by matplotlib directly below the code cell which generated those.\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from nose.tools import assert_equal\n",
    "\n",
    "EPSILON = .000001 # tiny tolerance for managing subtle differences resulting from floating point operations\n",
    "\n",
    "PART_1_DIR = \"part-1\"\n",
    "PART_2_DIR = \"part-2\"\n",
    "DOCUMENTS_FILE = PART_1_DIR+'/'+\"amazon_reviews.txt\"\n",
    "DATASET_FILE = PART_2_DIR+'/'+\"dataset.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "checksum": "d0b7c34e591d6f7d74ee3312b9b6813a",
     "grade": false,
     "grade_id": "part-1",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Part 1: General Coding (16 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "checksum": "94917d9d99795b6a80f51dc01bf0861f",
     "grade": false,
     "grade_id": "exercise-1-1-text",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Exercise 1.1 (4 points)\n",
    "\n",
    "Implement the function <code>**is_palindrome**</code> below, which takes as input a string and returns <code>**True**</code> iff that string is _palindrome_. A string is palindrome if it reads the same, both from left to right and from right to left, e.g., <code>**abba**</code>. Moreover, the empty string <code>**''**</code> is trivially palindrome.\n",
    "\n",
    "(**NOTE:** You can fairly assume the input string won't ever be_ <code>**None**</code> and won't contain any leading nor trailing whitespace characters.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "checksum": "11922832f6b67221f4bebfbc11f36e9f",
     "grade": false,
     "grade_id": "exercise-1-1-code",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def is_palindrome(string):\n",
    "    \"\"\"\n",
    "    Return True iff the input string is palindrome, False otherwise.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "checksum": "4cc98b563532212404d5dcaa8f73a538",
     "grade": true,
     "grade_id": "exercise-1-1-test",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Test the correctness of the implementation of the `is_palindrome` function\n",
    "\"\"\"\n",
    "\n",
    "# Tests\n",
    "assert_equal(True, is_palindrome(\"abba\"))\n",
    "assert_equal(False, is_palindrome(\"abracadabra\"))\n",
    "assert_equal(True, is_palindrome(\"\"))\n",
    "assert_equal(True, is_palindrome(\"test 2 tset\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "checksum": "7736edfe5a65d8b718ecede456a4ce6e",
     "grade": false,
     "grade_id": "exercise-1-2-text",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Exercise 1.2 (2 points)\n",
    "\n",
    "To solve this exercise, you will need to work with the file <code>**amazon_reviews.txt**</code>, which you will find inside <code style=\"color:blue\">**part-1**</code> directory. This file contains a sample of reviews provided by Amazon customers, with each line representing a single review (i.e., a text document).<br />\n",
    "\n",
    "As the first thing, implement the function <code>**create_bags_of_words**</code> which returns a dictionary. Each _key_ of this dictionary is the review identifier and the corresponding _value_ is the \"bag\" (i.e., list) of words contained in that review (document). For the sake of simplicity, you will use the line number as the document ID; for example, the document appearing at the first line of the file will have ID = 1, the second one ID = 2, and so on and so forth.\n",
    "Each bag of words should be obtained by _tokenizing_ (i.e., splitting) each line using the **whitespace character** (<code>**' '**</code>) as delimiter. In addition, all words in the list must be non-empty (<code>**''**</code>) and they should be properly \"normalized\", i.e., lowercased.\n",
    "\n",
    "(**NOTE:** You can assume each document will contain **at least one** non-empty word, i.e., eventually, there won't be any empty document.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "checksum": "420ac6fda316763a56ec5d1ca78640cc",
     "grade": false,
     "grade_id": "exercise-1-2-code",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def create_bags_of_words():\n",
    "    \"\"\"\n",
    "    Return a dictionary of the form {doc_id: [w_1, ..., w_n]} from the input file `amazon_reviews.txt`, where:\n",
    "    - doc_id is the number of line where the documents appear on the file;\n",
    "    - [w_1, ..., w_n] is the list of the non-empty words of doc_ID, properly lowercased.\n",
    "    \"\"\"\n",
    "    # This is the variable that should be returned\n",
    "    bags_of_words = {}\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    return bags_of_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "checksum": "c22dd8efdd25528746fdbbd7004ae186",
     "grade": true,
     "grade_id": "exercise-1-2-test",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Test the correctness of the implementation of the `create_bags_of_words` function\n",
    "\"\"\"\n",
    "\n",
    "# Call the function to test\n",
    "bags_of_words = create_bags_of_words()\n",
    "\n",
    "# Tests\n",
    "assert_equal(10, len(bags_of_words))\n",
    "assert_equal(106, len(bags_of_words[1]))\n",
    "assert_equal(148, len(bags_of_words[2]))\n",
    "assert_equal(60, len(bags_of_words[3]))\n",
    "assert_equal(37, len(bags_of_words[4]))\n",
    "assert_equal(['works', 'fine,', 'but', 'maha', 'energy', 'is', 'better:', \n",
    "              'check', 'out', 'maha', \"energy's\", 'website.', 'their', \n",
    "              'powerex', 'mh-c204f', 'charger', 'works', 'in', '100', \n",
    "              'minutes', 'for', 'rapid', 'charge,', 'with', 'option', \n",
    "              'for', 'slower', 'charge', '(better', 'for', 'batteries).', \n",
    "              'and', 'they', 'have', '2200', 'mah', 'batteries.']\n",
    "             , bags_of_words[4])\n",
    "assert_equal(['dvd', 'menu', 'select', 'problems:', 'i', 'cannot', 'scroll', \n",
    "              'through', 'a', 'dvd', 'menu', 'that', 'is', 'set', 'up', 'vertically.', \n",
    "              'the', 'triangle', 'keys', 'will', 'only', 'select', 'horizontally.', \n",
    "              'so', 'i', 'cannot', 'select', 'anything', 'on', 'most', \"dvd's\", 'besides', \n",
    "              'play.', 'no', 'special', 'features,', 'no', 'language', 'select,', 'nothing,', 'just', 'play.'], \n",
    "             bags_of_words[8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "checksum": "6776609ba2b2dd3f7a6535e2224f36b4",
     "grade": false,
     "grade_id": "exercise-1-3-text",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Exercise 1.2 (4 points)\n",
    "\n",
    "Implement the function <code>**clean_bag_of_words**</code> used by <code>**remove_special_characters**</code> below.  The <code>**clean_bag_of_words**</code> function takes as input a bag of words (i.e., a list of words) and returns a new list of _cleaned_ words. In particular, the function will remove any special character (e.g., <code>**.**</code>, <code>**?**</code>, <code>**$**</code>, etc.) appearing anywhere in each word of the original list. The word is kept in the bag iff it is still non-empy after special characters have been successfully removed. For example: \n",
    "-  <code>**'lunch!'**</code> ==> <code>**'lunch'**</code>\n",
    "-  <code>**'midd%le'**</code> ==> <code>**'middle'**</code>\n",
    "-  <code>**'?!?'**</code> ==> <code>**''**</code> (this cleaned token is empty and therewore won't be included in the bag of words)\n",
    "\n",
    "(**SUGGESTION:** Use <code>**set(string.punctuation)**</code>, as this contains the set of all special characters we are interested in.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "checksum": "e7986f7ce3ebd93c4a71fdb15d6be19b",
     "grade": false,
     "grade_id": "exercise-1-3-code",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def clean_bag_of_words(bag_of_words):\n",
    "    \"\"\"\n",
    "    Takes as input a list of words (bag) and returns a new list where original words\n",
    "    are cleaned, appropriately. Cleaning a word means removing from it *any* occurrence\n",
    "    of *any* special characters contained in the collection `set(string.punctuation)`.\n",
    "    Only non-empty cleaned words must be placed in the new, cleaned bag of words to be returned.\n",
    "    \"\"\"\n",
    "    # This is the variable that should be returned\n",
    "    cleaned_bag_of_words = []\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "\n",
    "def remove_special_characters(bags_of_words):\n",
    "    \"\"\"\n",
    "    This function makes use of `clean_bag_of_words` to clean every document (i.e., every list of words of a document)\n",
    "    \"\"\"\n",
    "    bags_of_cleaned_words = {}\n",
    "    for doc_id in bags_of_words:\n",
    "        # Call off to clean_bag_of_words passing the list of words of the current doc id as input\n",
    "        bags_of_cleaned_words[doc_id] = clean_bag_of_words(bags_of_words[doc_id])\n",
    "    return bags_of_cleaned_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "checksum": "9413c7fbf58f5b8b9dbc9a0bfbc08978",
     "grade": false,
     "grade_id": "exercise-1-3-required-test",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Set bags_of_words so as to allow you to run the tests even if you haven't implemented `create_bags_of_words`\n",
    "bags_of_words = {1: ['great', 'cd:', 'my', 'lovely', 'pat', 'has', 'one', 'of', 'the', 'great', 'voices', \n",
    "                     'of', 'her', 'generation.', 'i', 'have', 'listened', 'to', 'this', 'cd', 'for', 'years', \n",
    "                     'and', 'i', 'still', 'love', 'it.', 'when', \"i'm\", 'in', 'a', 'good', 'mood', 'it', \n",
    "                     'makes', 'me', 'feel', 'better.', 'a', 'bad', 'mood', 'just', 'evaporates', 'like', \n",
    "                     'sugar', 'in', 'the', 'rain.', 'this', 'cd', 'just', 'oozes', 'life.', 'vocals', 'are', \n",
    "                     'jusat', 'stuunning', 'and', 'lyrics', 'just', 'kill.', 'one', 'of', \"life's\", 'hidden', \n",
    "                     'gems.', 'this', 'is', 'a', 'desert', 'isle', 'cd', 'in', 'my', 'book.', 'why', 'she', \n",
    "                     'never', 'made', 'it', 'big', 'is', 'just', 'beyond', 'me.', 'everytime', 'i', 'play', \n",
    "                     'this,', 'no', 'matter', 'black,', 'white,', 'young,', 'old,', 'male,', 'female', \n",
    "                     'everybody', 'says', 'one', 'thing', '\"who', 'was', 'that', 'singing', '?\"'], \n",
    "                 2: ['one', 'of', 'the', 'best', 'game', 'music', 'soundtracks', '-', 'for', 'a', 'game', \n",
    "                     'i', \"didn't\", 'really', 'play:', 'despite', 'the', 'fact', 'that', 'i', 'have', \n",
    "                     'only', 'played', 'a', 'small', 'portion', 'of', 'the', 'game,', 'the', 'music', 'i', \n",
    "                     'heard', '(plus', 'the', 'connection', 'to', 'chrono', 'trigger', 'which', 'was', 'great', \n",
    "                     'as', 'well)', 'led', 'me', 'to', 'purchase', 'the', 'soundtrack,', 'and', 'it', 'remains', \n",
    "                     'one', 'of', 'my', 'favorite', 'albums.', 'there', 'is', 'an', 'incredible', 'mix', 'of', \n",
    "                     'fun,', 'epic,', 'and', 'emotional', 'songs.', 'those', 'sad', 'and', 'beautiful', \n",
    "                     'tracks', 'i', 'especially', 'like,', 'as', \"there's\", 'not', 'too', 'many', 'of', \n",
    "                     'those', 'kinds', 'of', 'songs', 'in', 'my', 'other', 'video', 'game', 'soundtracks.', \n",
    "                     'i', 'must', 'admit', 'that', 'one', 'of', 'the', 'songs', '(life-a', 'distant', 'promise)', \n",
    "                     'has', 'brought', 'tears', 'to', 'my', 'eyes', 'on', 'many', 'occasions.my', 'one', \n",
    "                     'complaint', 'about', 'this', 'soundtrack', 'is', 'that', 'they', 'use', 'guitar', \n",
    "                     'fretting', 'effects', 'in', 'many', 'of', 'the', 'songs,', 'which', 'i', 'find', \n",
    "                     'distracting.', 'but', 'even', 'if', 'those', \"weren't\", 'included', 'i', 'would', \n",
    "                     'still', 'consider', 'the', 'collection', 'worth', 'it.'], \n",
    "                 3: ['batteries', 'died', 'within', 'a', 'year', '...:', 'i', 'bought', 'this', 'charger', \n",
    "                     'in', 'jul', '2003', 'and', 'it', 'worked', 'ok', 'for', 'a', 'while.', 'the', \n",
    "                     'design', 'is', 'nice', 'and', 'convenient.', 'however,', 'after', 'about', 'a', \n",
    "                     'year,', 'the', 'batteries', 'would', 'not', 'hold', 'a', 'charge.', 'might', 'as', \n",
    "                     'well', 'just', 'get', 'alkaline', 'disposables,', 'or', 'look', 'elsewhere', 'for', \n",
    "                     'a', 'charger', 'that', 'comes', 'with', 'batteries', 'that', 'have', 'better', 'staying', \n",
    "                     'power.'], \n",
    "                 4: ['works', 'fine,', 'but', 'maha', 'energy', 'is', 'better:', 'check', \n",
    "                     'out', 'maha', \"energy's\", 'website.', 'their', 'powerex', 'mh-c204f', \n",
    "                     'charger', 'works', 'in', '100', 'minutes', 'for', 'rapid', 'charge,', 'with', \n",
    "                     'option', 'for', 'slower', 'charge', '(better', 'for', 'batteries).', 'and', 'they', \n",
    "                     'have', '2200', 'mah', 'batteries.'], \n",
    "                 5: ['great', 'for', 'the', 'non-audiophile:', 'reviewed', 'quite', 'a', 'bit', 'of', 'the', \n",
    "                     'combo', 'players', 'and', 'was', 'hesitant', 'due', 'to', 'unfavorable', 'reviews', \n",
    "                     'and', 'size', 'of', 'machines.', 'i', 'am', 'weaning', 'off', 'my', 'vhs', 'collection,', \n",
    "                     'but', \"don't\", 'want', 'to', 'replace', 'them', 'with', \"dvd's.\", 'this', 'unit', 'is', \n",
    "                     'well', 'built,', 'easy', 'to', 'setup', 'and', 'resolution', 'and', 'special', 'effects', \n",
    "                     '(no', 'progressive', 'scan', 'for', 'hdtv', 'owners)', 'suitable', 'for', 'many', 'people', \n",
    "                     'looking', 'for', 'a', 'versatile', 'product.cons-', 'no', 'universal', 'remote.'], \n",
    "                 6: ['dvd', 'player', 'crapped', 'out', 'after', 'one', 'year:', 'i', 'also', 'began', 'having', \n",
    "                     'the', 'incorrect', 'disc', 'problems', 'that', \"i've\", 'read', 'about', 'on', 'here.', \n",
    "                     'the', 'vcr', 'still', 'works,', 'but', 'hte', 'dvd', 'side', 'is', 'useless.', 'i', \n",
    "                     'understand', 'that', 'dvd', 'players', 'sometimes', 'just', 'quit', 'on', 'you,', 'but', \n",
    "                     'after', 'not', 'even', 'one', 'year?', 'to', 'me', \"that's\", 'a', 'sign', 'on', 'bad', \n",
    "                     'quality.', \"i'm\", 'giving', 'up', 'jvc', 'after', 'this', 'as', 'well.', \"i'm\", \n",
    "                     'sticking', 'to', 'sony', 'or', 'giving', 'another', 'brand', 'a', 'shot.'], \n",
    "                 7: ['incorrect', 'disc:', 'i', 'love', 'the', 'style', 'of', 'this,', 'but', 'after', 'a', \n",
    "                     'couple', 'years,', 'the', 'dvd', 'is', 'giving', 'me', 'problems.', 'it', \"doesn't\", \n",
    "                     'even', 'work', 'anymore', 'and', 'i', 'use', 'my', 'broken', 'ps2', 'now.', 'i', \n",
    "                     \"wouldn't\", 'recommend', 'this,', \"i'm\", 'just', 'going', 'to', 'upgrade', 'to', 'a', \n",
    "                     'recorder', 'now.', 'i', 'wish', 'it', 'would', 'work', 'but', 'i', 'guess', \"i'm\", \n",
    "                     'giving', 'up', 'on', 'jvc.', 'i', 'really', 'did', 'like', 'this', 'one...', 'before', \n",
    "                     'it', 'stopped', 'working.', 'the', 'dvd', 'player', 'gave', 'me', 'problems', 'probably', \n",
    "                     'after', 'a', 'year', 'of', 'having', 'it.'], \n",
    "                 8: ['dvd', 'menu', 'select', 'problems:', 'i', 'cannot', 'scroll', 'through', 'a', 'dvd', \n",
    "                     'menu', 'that', 'is', 'set', 'up', 'vertically.', 'the', 'triangle', 'keys', 'will', \n",
    "                     'only', 'select', 'horizontally.', 'so', 'i', 'cannot', 'select', 'anything', 'on', \n",
    "                     'most', \"dvd's\", 'besides', 'play.', 'no', 'special', 'features,', 'no', 'language', \n",
    "                     'select,', 'nothing,', 'just', 'play.'], \n",
    "                 9: ['unique', 'weird', 'orientalia', 'from', 'the', \"1930's:\", 'exotic', 'tales', 'of', \n",
    "                     'the', 'orient', 'from', 'the', \"1930's.\", '\"dr', 'shen', 'fu\",', 'a', 'weird', \n",
    "                     'tales', 'magazine', 'reprint,', 'is', 'about', 'the', 'elixir', 'of', 'life', 'that', \n",
    "                     'grants', 'immortality', 'at', 'a', 'price.', 'if', \"you're\", 'tired', 'of', 'modern', \n",
    "                     'authors', 'who', 'all', 'sound', 'alike,', 'this', 'is', 'the', 'antidote', 'for', \n",
    "                     'you.', \"owen's\", 'palette', 'is', 'loaded', 'with', 'splashes', 'of', 'chinese', 'and', \n",
    "                     'japanese', 'colours.', 'marvelous.'], \n",
    "                 10: ['not', 'an', '\"ultimate', 'guide\":', 'firstly,i', 'enjoyed', 'the', 'format', 'and', \n",
    "                      'tone', 'of', 'the', 'book', '(how', 'the', 'author', 'addressed', 'the', 'reader).', \n",
    "                      'however,', 'i', 'did', 'not', 'feel', 'that', 'she', 'imparted', 'any', 'insider', \n",
    "                      'secrets', 'that', 'the', 'book', 'promised', 'to', 'reveal.', 'if', 'you', 'are', \n",
    "                      'just', 'starting', 'to', 'research', 'law', 'school,', 'and', 'do', 'not', 'know', \n",
    "                      'all', 'the', 'requirements', 'of', 'admission,', 'then', 'this', 'book', 'may', 'be', \n",
    "                      'a', 'tremendous', 'help.', 'if', 'you', 'have', 'done', 'your', 'homework', 'and', \n",
    "                      'are', 'looking', 'for', 'an', 'edge', 'when', 'it', 'comes', 'to', 'admissions,', 'i', \n",
    "                      'recommend', 'some', 'more', 'topic-specific', 'books.', 'for', 'example,', 'books', \n",
    "                      'on', 'how', 'to', 'write', 'your', 'personal', 'statment,', 'books', 'geared', \n",
    "                      'specifically', 'towards', 'lsat', 'preparation', '(powerscore', 'books', 'were', \n",
    "                      'the', 'most', 'helpful', 'for', 'me),', 'and', 'there', 'are', 'some', 'websites', \n",
    "                      'with', 'great', 'advice', 'geared', 'towards', 'aiding', 'the', 'individuals', \n",
    "                      'whom', 'you', 'are', 'asking', 'to', 'write', 'letters', 'of', 'recommendation.', \n",
    "                      'yet,', 'for', 'those', 'new', 'to', 'the', 'entire', 'affair,', 'this', 'book', \n",
    "                      'can', 'definitely', 'clarify', 'the', 'requirements', 'for', 'you.']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "checksum": "827d86b5680c66df76a6100a7c776057",
     "grade": true,
     "grade_id": "exercise-1-3-test",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Test the correctness of the implementation of the `clean_bag_of_words` function\n",
    "\"\"\"\n",
    "\n",
    "# Call the function to test\n",
    "cleaned_bags_of_words = remove_special_characters(bags_of_words)\n",
    "\n",
    "# Tests\n",
    "assert_equal(10, len(cleaned_bags_of_words))\n",
    "assert_equal(105, len(cleaned_bags_of_words[1]))\n",
    "assert_equal(59, len(cleaned_bags_of_words[3]))\n",
    "assert_equal(['one', 'of', 'the', 'best', 'game', 'music', 'soundtracks', 'for', 'a', 'game', \n",
    "              'i', 'didnt', 'really', 'play', 'despite', 'the', 'fact', 'that', 'i', 'have', \n",
    "              'only', 'played', 'a', 'small', 'portion', 'of', 'the', 'game', 'the', 'music', 'i', \n",
    "              'heard', 'plus', 'the', 'connection', 'to', 'chrono', 'trigger', 'which', 'was', \n",
    "              'great', 'as', 'well', 'led', 'me', 'to', 'purchase', 'the', 'soundtrack', 'and', \n",
    "              'it', 'remains', 'one', 'of', 'my', 'favorite', 'albums', 'there', 'is', 'an', \n",
    "              'incredible', 'mix', 'of', 'fun', 'epic', 'and', 'emotional', 'songs', 'those', 'sad', \n",
    "              'and', 'beautiful', 'tracks', 'i', 'especially', 'like', 'as', 'theres', 'not', 'too', \n",
    "              'many', 'of', 'those', 'kinds', 'of', 'songs', 'in', 'my', 'other', 'video', 'game', \n",
    "              'soundtracks', 'i', 'must', 'admit', 'that', 'one', 'of', 'the', 'songs', 'lifea', \n",
    "              'distant', 'promise', 'has', 'brought', 'tears', 'to', 'my', 'eyes', 'on', 'many', \n",
    "              'occasionsmy', 'one', 'complaint', 'about', 'this', 'soundtrack', 'is', 'that', 'they', \n",
    "              'use', 'guitar', 'fretting', 'effects', 'in', 'many', 'of', 'the', 'songs', 'which', \n",
    "              'i', 'find', 'distracting', 'but', 'even', 'if', 'those', 'werent', 'included', 'i', \n",
    "              'would', 'still', 'consider', 'the', 'collection', 'worth', 'it'], \n",
    "             cleaned_bags_of_words[2])\n",
    "assert_equal(['dvd', 'menu', 'select', 'problems', 'i', 'cannot', 'scroll', 'through', 'a', 'dvd', \n",
    "              'menu', 'that', 'is', 'set', 'up', 'vertically', 'the', 'triangle', 'keys', 'will', \n",
    "              'only', 'select', 'horizontally', 'so', 'i', 'cannot', 'select', 'anything', 'on', 'most', \n",
    "              'dvds', 'besides', 'play', 'no', 'special', 'features', 'no', 'language', 'select', 'nothing', \n",
    "              'just', 'play'], \n",
    "             cleaned_bags_of_words[8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "checksum": "6527ff8d69c1c1fa6b0f9f1ec6cba8ae",
     "grade": false,
     "grade_id": "exercise-1-4-text",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Exercise 1.3 (7 points)\n",
    "\n",
    "Implement the **4** functions below:\n",
    "-  <code>**term_freq**</code> \n",
    "-  <code>**create_idf_vector**</code>\n",
    "-  <code>**create_doc_tf_vector**</code>\n",
    "-  <code>**create_doc_term_matrix_tf_idf**</code>\n",
    "\n",
    "Each function above is a small brick which is used to achieve the overall task of building a document-term matrix using the _tf-idf_ weighting scheme. To simplify this, you are given a useful data structure called **inverted_index**, which has been built in advance for you.\n",
    "A **document-term matrix** is an $m$-by-$n$ matrix where $m$ is the number of documents in the collection and $n$ the number of all terms (i.e., the size of the **vocabulary**). Each row of the matrix is therefore a document; each document is in turn represented by an $n$-dimensional vector (i.e., one dimension for each term in the vocabulary).\n",
    "Each dimension of a document vector is computed using the $tf$-$idf$ score, which is computed as the product of two scores: $tf$-$idf$ = $tf$ * $idf$. In other words, given the document $d$ its corresponding vector is computed as follows:\n",
    "$$\n",
    "d[i] = tf(i, d) * idf(i),~i\\in {1, \\ldots, n}\n",
    "$$\n",
    "\n",
    "The first score $tf(i, d)$ is computed _locally_ to the document and measures the frequency of term $i$ in document $d$ (it will be 0 if $d$ does not contain $i$). The second score $idf(i)$, instead, measures the (smoothed) *inverse document frequency* of the term $i$. This is computed as follows:\n",
    "\n",
    "$$\n",
    "idf(i) = \\log \\Big(1 + \\frac{m}{m_i}\\Big)\n",
    "$$\n",
    "where $m$ is the total number of documents in the collection and $m_i$ is the number of documents containing the term $i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "checksum": "8fb9ffb6947b5b0f60f2fa43a2af86f2",
     "grade": false,
     "grade_id": "exercise-1-4-code",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def term_freq(term, doc_id, inverted_index):\n",
    "    \"\"\"\n",
    "    Takes as input a term, a doc id and the inverted index.\n",
    "    Returns an integer representing the frequency of the input term in the input doc id, using the inverted index.\n",
    "    If the term is not present in the inverted index, the function returns None.\n",
    "    If the term is in the inverted index but it is not in the doc id, the function returns 0.\n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "\n",
    "def create_idf_vector(n_docs, vocabulary, inverted_index):\n",
    "    \"\"\"\n",
    "    Return a \"global\" idf vector, i.e., an n-dimensional numpy array where n = |vocabulary|.\n",
    "    Each entry of this vector is the idf score of a vocabulary term.\n",
    "    Remember that (smoothed) inverse document frequency of a term is computed as follows:\n",
    "    idf(term) = log(1 + m/m_term) where m = |documents|, m_term = |documents containing term|\n",
    "    To do so, the function takes as input:\n",
    "    - m = n_docs (the total number of documents in the collection),\n",
    "    - vocabulary (the list of terms extracted from the whole collection of documents)\n",
    "    - inverted_index (a dictionary that maps each term of the vocabulary into another dictionary\n",
    "    containing the doc id as key and the term frequency as value).\n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR CODE HERE    \n",
    "    \n",
    "\n",
    "def create_doc_tf_vector(doc_id, vocabulary, inverted_index):\n",
    "    \"\"\"\n",
    "    Return a \"local\" term frequency vector representing doc id, \n",
    "    i.e., an n-dimensional numpy array where n = |vocabulary|.\n",
    "    Use the vocabulary and the inverted index.\n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    \n",
    "\n",
    "def create_doc_term_matrix_tf_idf(cleaned_bags_of_words, vocabulary, inverted_index):\n",
    "    \"\"\"\n",
    "    Return a document-term matrix, i.e., an m-by-n numpy array where m = |documents|, n = |vocabulary|\n",
    "    Each row of the matrix corresponds to the tf-idf vector of a document.\n",
    "    A tf-idf vector is just the result of the element-wise multiplication of the tf vector (of a document)\n",
    "    and the idf vector (which is the same for *all* documents).\n",
    "    \"\"\"\n",
    "    \n",
    "    # Number m of documents\n",
    "    m = len(cleaned_bags_of_words)\n",
    "    # Number n of terms in the vocabulary\n",
    "    n = len(vocabulary)\n",
    "    \n",
    "    # Initialize the document-term matrix to an empty list \n",
    "    # (eventually, it will be a list of list where each list is a row of the document-term matrix)\n",
    "    tf_idf_doc_term_matrix = []\n",
    "    \n",
    "    # Compute the global idf vector for all the terms in the vocabulary (once for all)\n",
    "    idf_vector = create_idf_vector(m, vocabulary, inverted_index)\n",
    "    \n",
    "    # Compute the tf-idf vector for each document of the collection (i.e., each row of the matrix)\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    return np.array(tf_idf_doc_term_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "checksum": "594e75041384105d14124df28339e426",
     "grade": false,
     "grade_id": "exercise-1-4-required-test",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Function used to create the inverted index\n",
    "def create_inverted_index_term_freq(cleaned_bags_of_words):\n",
    "    \"\"\"\n",
    "    Return the inverted index built from the bags of words representing the collection of documents.\n",
    "    The inverted index contains an entry for each word (term) appearing in the collection; \n",
    "    for each word, it contains a dictionary whose key is the document ID where that word appears \n",
    "    and the value is the number of times (i.e., frequency) of that term in that document.\n",
    "    Example:\n",
    "    {\n",
    "    'ananas': {3: 1, 6: 2},\n",
    "    ...\n",
    "    'zoo': {10:5}\n",
    "    }\n",
    "    In the example above, the term 'ananas' appears 1 time in doc ID 3 and 2 times in doc ID 6.\n",
    "    The term 'zoo', instead, appears 5 times in doc ID 10.\n",
    "    \"\"\"\n",
    "    inverted_index = {}\n",
    "    for doc_id, words in cleaned_bags_of_words.items(): \n",
    "        word_freq = {}\n",
    "        for word in words:\n",
    "            word_freq[word] = word_freq.get(word, 0) + 1\n",
    "            if word in inverted_index:\n",
    "                inverted_index[word][doc_id] = word_freq[word]\n",
    "            else:\n",
    "                inverted_index[word] = {}\n",
    "                inverted_index[word][doc_id] = word_freq[word]\n",
    "    return inverted_index\n",
    "\n",
    "\n",
    "# Set cleaned_bags_of_words so as to allow you to run the tests even if you haven't implemented `clean_bag_of_words`\n",
    "cleaned_bags_of_words = {1: ['great', 'cd', 'my', 'lovely', 'pat', 'has', 'one', 'of', 'the', 'great', 'voices', \n",
    "                             'of', 'her', 'generation', 'i', 'have', 'listened', 'to', 'this', 'cd', 'for', \n",
    "                             'years', 'and', 'i', 'still', 'love', 'it', 'when', 'im', 'in', 'a', 'good', \n",
    "                             'mood', 'it', 'makes', 'me', 'feel', 'better', 'a', 'bad', 'mood', 'just', 'evaporates', \n",
    "                             'like', 'sugar', 'in', 'the', 'rain', 'this', 'cd', 'just', 'oozes', 'life', 'vocals', \n",
    "                             'are', 'jusat', 'stuunning', 'and', 'lyrics', 'just', 'kill', 'one', 'of', 'lifes', \n",
    "                             'hidden', 'gems', 'this', 'is', 'a', 'desert', 'isle', 'cd', 'in', 'my', 'book', \n",
    "                             'why', 'she', 'never', 'made', 'it', 'big', 'is', 'just', 'beyond', 'me', 'everytime', \n",
    "                             'i', 'play', 'this', 'no', 'matter', 'black', 'white', 'young', 'old', 'male', \n",
    "                             'female', 'everybody', 'says', 'one', 'thing', 'who', 'was', 'that', 'singing'], \n",
    "                         2: ['one', 'of', 'the', 'best', 'game', 'music', 'soundtracks', 'for', 'a', 'game', 'i', \n",
    "                             'didnt', 'really', 'play', 'despite', 'the', 'fact', 'that', 'i', 'have', 'only', \n",
    "                             'played', 'a', 'small', 'portion', 'of', 'the', 'game', 'the', 'music', 'i', 'heard', \n",
    "                             'plus', 'the', 'connection', 'to', 'chrono', 'trigger', 'which', 'was', 'great', 'as', \n",
    "                             'well', 'led', 'me', 'to', 'purchase', 'the', 'soundtrack', 'and', 'it', 'remains', \n",
    "                             'one', 'of', 'my', 'favorite', 'albums', 'there', 'is', 'an', 'incredible', 'mix', \n",
    "                             'of', 'fun', 'epic', 'and', 'emotional', 'songs', 'those', 'sad', 'and', 'beautiful', \n",
    "                             'tracks', 'i', 'especially', 'like', 'as', 'theres', 'not', 'too', 'many', 'of', \n",
    "                             'those', 'kinds', 'of', 'songs', 'in', 'my', 'other', 'video', 'game', 'soundtracks', \n",
    "                             'i', 'must', 'admit', 'that', 'one', 'of', 'the', 'songs', 'lifea', 'distant', 'promise', \n",
    "                             'has', 'brought', 'tears', 'to', 'my', 'eyes', 'on', 'many', 'occasionsmy', 'one', \n",
    "                             'complaint', 'about', 'this', 'soundtrack', 'is', 'that', 'they', 'use', 'guitar', \n",
    "                             'fretting', 'effects', 'in', 'many', 'of', 'the', 'songs', 'which', 'i', 'find', \n",
    "                             'distracting', 'but', 'even', 'if', 'those', 'werent', 'included', 'i', 'would', \n",
    "                             'still', 'consider', 'the', 'collection', 'worth', 'it'], \n",
    "                         3: ['batteries', 'died', 'within', 'a', 'year', 'i', 'bought', 'this', 'charger', 'in', \n",
    "                             'jul', '2003', 'and', 'it', 'worked', 'ok', 'for', 'a', 'while', 'the', 'design', \n",
    "                             'is', 'nice', 'and', 'convenient', 'however', 'after', 'about', 'a', 'year', 'the', \n",
    "                             'batteries', 'would', 'not', 'hold', 'a', 'charge', 'might', 'as', 'well', 'just', \n",
    "                             'get', 'alkaline', 'disposables', 'or', 'look', 'elsewhere', 'for', 'a', 'charger', \n",
    "                             'that', 'comes', 'with', 'batteries', 'that', 'have', 'better', 'staying', 'power'], \n",
    "                         4: ['works', 'fine', 'but', 'maha', 'energy', 'is', 'better', 'check', 'out', 'maha', \n",
    "                             'energys', 'website', 'their', 'powerex', 'mhc204f', 'charger', 'works', 'in', '100', \n",
    "                             'minutes', 'for', 'rapid', 'charge', 'with', 'option', 'for', 'slower', 'charge', \n",
    "                             'better', 'for', 'batteries', 'and', 'they', 'have', '2200', 'mah', 'batteries'], \n",
    "                         5: ['great', 'for', 'the', 'nonaudiophile', 'reviewed', 'quite', 'a', 'bit', 'of', \n",
    "                             'the', 'combo', 'players', 'and', 'was', 'hesitant', 'due', 'to', 'unfavorable', \n",
    "                             'reviews', 'and', 'size', 'of', 'machines', 'i', 'am', 'weaning', 'off', 'my', \n",
    "                             'vhs', 'collection', 'but', 'dont', 'want', 'to', 'replace', 'them', 'with', \n",
    "                             'dvds', 'this', 'unit', 'is', 'well', 'built', 'easy', 'to', 'setup', 'and', \n",
    "                             'resolution', 'and', 'special', 'effects', 'no', 'progressive', 'scan', 'for', \n",
    "                             'hdtv', 'owners', 'suitable', 'for', 'many', 'people', 'looking', 'for', 'a', \n",
    "                             'versatile', 'productcons', 'no', 'universal', 'remote'], \n",
    "                         6: ['dvd', 'player', 'crapped', 'out', 'after', 'one', 'year', 'i', 'also', 'began', \n",
    "                             'having', 'the', 'incorrect', 'disc', 'problems', 'that', 'ive', 'read', 'about', \n",
    "                             'on', 'here', 'the', 'vcr', 'still', 'works', 'but', 'hte', 'dvd', 'side', 'is', \n",
    "                             'useless', 'i', 'understand', 'that', 'dvd', 'players', 'sometimes', 'just', 'quit', \n",
    "                             'on', 'you', 'but', 'after', 'not', 'even', 'one', 'year', 'to', 'me', 'thats', 'a', \n",
    "                             'sign', 'on', 'bad', 'quality', 'im', 'giving', 'up', 'jvc', 'after', 'this', 'as', \n",
    "                             'well', 'im', 'sticking', 'to', 'sony', 'or', 'giving', 'another', 'brand', 'a', 'shot'], \n",
    "                         7: ['incorrect', 'disc', 'i', 'love', 'the', 'style', 'of', 'this', 'but', 'after', 'a', \n",
    "                             'couple', 'years', 'the', 'dvd', 'is', 'giving', 'me', 'problems', 'it', 'doesnt', 'even', \n",
    "                             'work', 'anymore', 'and', 'i', 'use', 'my', 'broken', 'ps2', 'now', 'i', 'wouldnt', \n",
    "                             'recommend', 'this', 'im', 'just', 'going', 'to', 'upgrade', 'to', 'a', 'recorder', \n",
    "                             'now', 'i', 'wish', 'it', 'would', 'work', 'but', 'i', 'guess', 'im', 'giving', 'up', \n",
    "                             'on', 'jvc', 'i', 'really', 'did', 'like', 'this', 'one', 'before', 'it', 'stopped', \n",
    "                             'working', 'the', 'dvd', 'player', 'gave', 'me', 'problems', 'probably', 'after', 'a', \n",
    "                             'year', 'of', 'having', 'it'], \n",
    "                         8: ['dvd', 'menu', 'select', 'problems', 'i', 'cannot', 'scroll', 'through', 'a', 'dvd', \n",
    "                             'menu', 'that', 'is', 'set', 'up', 'vertically', 'the', 'triangle', 'keys', 'will', \n",
    "                             'only', 'select', 'horizontally', 'so', 'i', 'cannot', 'select', 'anything', 'on', \n",
    "                             'most', 'dvds', 'besides', 'play', 'no', 'special', 'features', 'no', 'language', \n",
    "                             'select', 'nothing', 'just', 'play'], \n",
    "                         9: ['unique', 'weird', 'orientalia', 'from', 'the', '1930s', 'exotic', 'tales', 'of', \n",
    "                             'the', 'orient', 'from', 'the', '1930s', 'dr', 'shen', 'fu', 'a', 'weird', 'tales', \n",
    "                             'magazine', 'reprint', 'is', 'about', 'the', 'elixir', 'of', 'life', 'that', 'grants', \n",
    "                             'immortality', 'at', 'a', 'price', 'if', 'youre', 'tired', 'of', 'modern', 'authors', \n",
    "                             'who', 'all', 'sound', 'alike', 'this', 'is', 'the', 'antidote', 'for', 'you', 'owens', \n",
    "                             'palette', 'is', 'loaded', 'with', 'splashes', 'of', 'chinese', 'and', 'japanese', \n",
    "                             'colours', 'marvelous'], \n",
    "                         10: ['not', 'an', 'ultimate', 'guide', 'firstlyi', 'enjoyed', 'the', 'format', 'and', \n",
    "                              'tone', 'of', 'the', 'book', 'how', 'the', 'author', 'addressed', 'the', 'reader', \n",
    "                              'however', 'i', 'did', 'not', 'feel', 'that', 'she', 'imparted', 'any', 'insider', \n",
    "                              'secrets', 'that', 'the', 'book', 'promised', 'to', 'reveal', 'if', 'you', 'are', \n",
    "                              'just', 'starting', 'to', 'research', 'law', 'school', 'and', 'do', 'not', 'know', \n",
    "                              'all', 'the', 'requirements', 'of', 'admission', 'then', 'this', 'book', 'may', 'be', \n",
    "                              'a', 'tremendous', 'help', 'if', 'you', 'have', 'done', 'your', 'homework', 'and', \n",
    "                              'are', 'looking', 'for', 'an', 'edge', 'when', 'it', 'comes', 'to', 'admissions', \n",
    "                              'i', 'recommend', 'some', 'more', 'topicspecific', 'books', 'for', 'example', 'books', \n",
    "                              'on', 'how', 'to', 'write', 'your', 'personal', 'statment', 'books', 'geared', \n",
    "                              'specifically', 'towards', 'lsat', 'preparation', 'powerscore', 'books', 'were', \n",
    "                              'the', 'most', 'helpful', 'for', 'me', 'and', 'there', 'are', 'some', 'websites', \n",
    "                              'with', 'great', 'advice', 'geared', 'towards', 'aiding', 'the', 'individuals', \n",
    "                              'whom', 'you', 'are', 'asking', 'to', 'write', 'letters', 'of', 'recommendation', \n",
    "                              'yet', 'for', 'those', 'new', 'to', 'the', 'entire', 'affair', 'this', 'book', 'can', \n",
    "                              'definitely', 'clarify', 'the', 'requirements', 'for', 'you']}\n",
    "\n",
    "# Make use of the function for creating the inverted index (already implemented)\n",
    "inverted_index = create_inverted_index_term_freq(cleaned_bags_of_words)\n",
    "\n",
    "# Extract the vocabulary from the inverted index\n",
    "vocabulary = sorted(inverted_index.keys())\n",
    "\n",
    "# Number of documents in the collection\n",
    "m = len(cleaned_bags_of_words)\n",
    "\n",
    "# Number n of terms in the vocabulary\n",
    "n = len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "checksum": "b482112c3ed7f613b9a331d242669f69",
     "grade": true,
     "grade_id": "exercise-1-4-test-term-freq",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Test the correctness of the implementation of the `term_freq` function\n",
    "\"\"\"\n",
    "\n",
    "# Tests\n",
    "# Correct term frequency\n",
    "assert_equal(1, term_freq(\"despite\", 2, inverted_index))\n",
    "# Correct term frequency\n",
    "assert_equal(4, term_freq(\"cd\", 1, inverted_index))\n",
    "# Term frequency of a word that is not in the inverted index should return None\n",
    "assert_equal(None, term_freq(\"apple\", 4, inverted_index))\n",
    "# Term frequency of a word that IS in the inverted index but not in the input document should return 0\n",
    "assert_equal(0, term_freq(\"rapid\", 9, inverted_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "checksum": "775b12353cb06e370d749b024b342f09",
     "grade": true,
     "grade_id": "exercise-1-4-test-create-idf-vector",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Test the correctness of the implementation of the `create_idf_vector` function\n",
    "\"\"\"\n",
    "\n",
    "# Tests\n",
    "# Correct idf vector length\n",
    "assert_equal(n, create_idf_vector(m, vocabulary, inverted_index).size)\n",
    "# Correct idf vector (first 10 entries)\n",
    "assert_equal(True, \n",
    "             np.all(\n",
    "                 np.abs(\n",
    "                     np.array([2.39789527,  2.39789527,  2.39789527,  2.39789527,  0.7472144 ,\n",
    "                               1.25276297,  2.39789527,  2.39789527,  2.39789527,  2.39789527]) - \n",
    "                     create_idf_vector(m, vocabulary, inverted_index)[:10]\n",
    "                 )\n",
    "                 < EPSILON)\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "checksum": "2333dae3c4d86a269b1871882196da95",
     "grade": true,
     "grade_id": "exercise-1-4-test-create-doc-tf-vector",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Test the correctness of the implementation of the `create_doc_tf_vector` function\n",
    "(depends on `term_freq`)\n",
    "\"\"\"\n",
    "\n",
    "# Tests\n",
    "# Correct document vector length\n",
    "assert_equal([n for i in range(1, m+1)], \n",
    "             [create_doc_tf_vector(i, vocabulary, inverted_index).size for i in range(1,m+1)]\n",
    "            )\n",
    "# Correct document ID 2 term-frequency vector (first 10 entries)\n",
    "assert_equal(True, \n",
    "             np.all(np.array([0, 0, 0, 0, 2, 1, 0, 0, 0, 1]) \n",
    "                    == \n",
    "                    create_doc_tf_vector(2, vocabulary, inverted_index)[:10])\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "checksum": "9a61742f875df739f16037540759ddcc",
     "grade": true,
     "grade_id": "exercise-1-4-test-create-doc-term-matrix-tf-idf",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Test the correctness of the implementation of the `create_doc_term_matrix_tf_idf` function\n",
    "\"\"\"\n",
    "\n",
    "# Call the function\n",
    "matrix = create_doc_term_matrix_tf_idf(cleaned_bags_of_words, vocabulary, inverted_index)\n",
    "\n",
    "# Tests\n",
    "# Correct shape of the resulting matrix\n",
    "assert_equal((m, n), matrix.shape)\n",
    "# Correct (i,j) entry of the matrix corresponding to doc ID 6 and 43rd term\n",
    "assert_equal(0, matrix[5, 42])\n",
    "# Correct (i,j) entry of the matrix corresponding to doc ID 3 and 13rd term\n",
    "assert_equal(True, (1.4663370687934272 - matrix[2, 12] < EPSILON))\n",
    "# Correct document ID 7 tf-idf vector (first 20 entries)\n",
    "assert_equal(True, \n",
    "             np.all(\n",
    "                 np.abs(\n",
    "                     np.array([0.        ,  0.        ,  0.        ,  0.        ,  2.24164321,\n",
    "                              0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
    "                              0.        ,  0.        ,  2.93267414,  0.        ,  0.        ,\n",
    "                              0.        ,  0.        ,  0.        ,  0.        ,  0.        ]) -\n",
    "                     matrix[6][:20]\n",
    "                 )\n",
    "                 < EPSILON)\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "checksum": "dd61cc54a034a224e32c179bf020ca01",
     "grade": false,
     "grade_id": "part-2",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Part 2: Data Science (16 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "checksum": "aae0d6417cd6e14b52321e50943ea178",
     "grade": false,
     "grade_id": "part-2-preliminary",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Load the dataset stored at `DATASET_FILE` using \",\" as field separator\n",
    "data = pd.read_csv(DATASET_FILE, \n",
    "                   sep=\",\", \n",
    "                   header=None, \n",
    "                   names=['f{}'.format(i) for i in range(1, 16)]+['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "checksum": "ee0fcf1dc139c9fd7c772a134bba0f85",
     "grade": false,
     "grade_id": "exercise-2-1-text",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Exercise 2.1 (2 points)\n",
    "\n",
    "Implement the function <code>**modify_dataset**</code> below, which is responsible for modifying the <code>**pandas.DataFrame**</code> object loaded in the cell right above.<br />\n",
    "Such a dataset has **37 instances** which contain **at least one** missing value (indicated with the <code>**'?'**</code> symbol) and spread across **7 features** (i.e., columns), according to the <code>**README.txt**</code>. You should properly change those fields denoted by <code>**'?'**</code> into actual missing values.<br />\n",
    "Finally, you are asked to substitute <code>**'+'**</code>/<code>**'-'**</code> values of the <code>**label**</code> column with <code>**1**</code>/<code>**-1**</code>, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "checksum": "5503ed26bf48999869558e5c825f3f31",
     "grade": false,
     "grade_id": "exercise-2-1-code",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def modify_dataset(data):\n",
    "    \"\"\"\n",
    "    Deal with missing header row by providing one and handle missing values denoted by '?'.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "checksum": "dfe6af1c07172982ae360c3917f292b4",
     "grade": true,
     "grade_id": "exercise-2-1-test",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Test the correctness of the implementation of the `load_dataset` function\n",
    "\"\"\"\n",
    "\n",
    "data = modify_dataset(data)\n",
    "\n",
    "assert_equal((690, 16), data.shape)\n",
    "assert_equal(True, data.isnull().any().any())\n",
    "assert_equal(37, data.isnull().any(axis=1).sum())\n",
    "assert_equal(['f1', 'f2', 'f3', 'f4', 'f5', 'f6', 'f7', 'f8', \n",
    "             'f9', 'f10', 'f11', 'f12', 'f13', 'f14', 'f15', 'label'],\n",
    "            list(data.columns.values))\n",
    "assert_equal(307, data[data.label == 1].shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "checksum": "6f84050f171495c9e7d554970002469e",
     "grade": false,
     "grade_id": "exercise-2-2-text",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Exercise 2.2 (3 points)\n",
    "\n",
    "Implement the function <code>**fill_missing_value**</code> below. This takes as input a <code>**pandas.DataFrame**</code> object and returns a new <code>**pandas.DataFrame**</code> where **all** existing missing values are replaced according to the following rule:\n",
    "-  If a missing value refers to a **numerical column**, then it must be replaced with the **median** computed across that column\n",
    "-  If a missing value refers to a **categorical column**, then it must be replaced with the **mode** computed across that column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "checksum": "a7abf4a33969491a362dad1fd58e923b",
     "grade": false,
     "grade_id": "exercise-2-2-code",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# is_numeric_dtype(pandas.Series) returns True iff the dtype associated with the pandas.Series passed as input is numeric\n",
    "from pandas.api.types import is_numeric_dtype\n",
    "\n",
    "def fill_missing_values(data):\n",
    "    \"\"\"\n",
    "    Input: pandas.DataFrame (data)\n",
    "    Output: pandas.DataFrame\n",
    "    \n",
    "    Return a DataFrame where ALL existing missing values are replaced according to the following rule:\n",
    "    - If a missing value refers to a numerical column, then it must be replaced with the median of that column\n",
    "    - If a missing value refers to a categorical column, then it must be replaced with the mode of that column\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "checksum": "23d43c7b29da5e817cad8e8adb3b8481",
     "grade": true,
     "grade_id": "exercise-2-2-test",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Test the correctness of the implementation of the `fill_missing_values` function\n",
    "\"\"\"\n",
    "\n",
    "data = fill_missing_values(data)\n",
    "\n",
    "assert_equal(False, data.isnull().any().any())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "checksum": "bb7980980276493540a5facda692f42c",
     "grade": false,
     "grade_id": "exercise-2-3-text",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Exercise 2.3 (3 points)\n",
    "\n",
    "Implement the function <code>**slice_data**</code> below. This takes as input a <code>**pandas.DataFrame**</code> object and returns a _sliced_ <code>**pandas.DataFrame**</code> which contains **all** the _positive_ instances whose value of the feature labeled as <code>**f3**</code> is greater than the mean of _that_ feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "checksum": "cc9a48ba501a73ac52cca9e2304f75fc",
     "grade": false,
     "grade_id": "exercise-2-3-code",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def slice_data(data):\n",
    "    \"\"\"\n",
    "    Input: pandas.DataFrame (data)\n",
    "    Output: pandas.DataFrame\n",
    "    \n",
    "    Return a sliced DataFrame which contains **all** the positive instances \n",
    "    whose value of the feature labeled as <code>**f3**</code> is greater than the mean of that feature\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    mean = data['f3'].mean()\n",
    "    mask = (data['label'] > 0) & (data['f3'] > mean)\n",
    "    return data[mask]\n",
    "    \n",
    "    #raise NotImplementedError()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "checksum": "a1bbf8b3ff0ca6d7724bdfbfad80404d",
     "grade": true,
     "grade_id": "exercise-2-3-test",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Test the correctness of the implementation of the `slice_data` function\n",
    "\"\"\"\n",
    "\n",
    "sliced_data = slice_data(data)\n",
    "\n",
    "assert_equal(True, (sliced_data.f3 > 4.7587246376811585).all())\n",
    "assert_equal(True, (sliced_data.label == 1).all())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "checksum": "2269cebd78b9d81eee9c61f760bcfd4e",
     "grade": false,
     "grade_id": "exercise-2-4-text",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Exercise 2.4 (6 points)\n",
    "\n",
    "This exercise is made of **3** questions, which you can answer independently to each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "checksum": "e0e9335ae7bd2e387f0886e1ad527b23",
     "grade": false,
     "grade_id": "exercise-2-4-1-text",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Question 1 (1 point)\n",
    "\n",
    "Feature labeled as <code>**f5**</code> represents a categorical variable which can take on **3** distinct values: <code>**g**</code>, <code>**p**</code>, and <code>**g**</code>. Assign to the variable <code>**count_p**</code> below the total number of instances in the dataset whose value of <code>**f5**</code> is <code>**p**</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "checksum": "c59ba268256ce6b45759147fb2eeb529",
     "grade": false,
     "grade_id": "exercise-2-4-1-code",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "count_p = None\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "checksum": "fd95fdfe42c4c8744f7053a5dbe11497",
     "grade": true,
     "grade_id": "exercise-2-4-1-test",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Test the correctness of the `count_p`\n",
    "\"\"\"\n",
    "\n",
    "assert_equal(False, (count_p == None))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "checksum": "b0674495984eddb614ccdcea32948c84",
     "grade": false,
     "grade_id": "exercise-2-4-2-text",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Question 2 (1 point)\n",
    "\n",
    "Plot the distribution of feature <code>**f8**</code> (i.e., using <code>**seaborn.distplot**</code> method, along with its density estimate) and assign the result of the plot to the variable <code>**dist_plot_f8**</code>. In addition to that, set the variable <code>**is_f8_normal**</code> to either <code>**True**</code> or <code>**False**</code> depending on whether the distribution you obtain looks more like a Gaussian (Normal) distribution or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "checksum": "1dc58c49096d72b5b8be1a4075b346c3",
     "grade": false,
     "grade_id": "exercise-2-4-2-code",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "dist_plot_f8 = None\n",
    "is_f8_normal = None\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "checksum": "0fd0cbd15a2bf4636f92893d9b072ef4",
     "grade": true,
     "grade_id": "exercise-2-4-2-test",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Test the correctness of `dist_plot_f8` and `is_f8_normal`\n",
    "\"\"\"\n",
    "\n",
    "assert_equal(False, (dist_plot_f8 == None))\n",
    "assert_equal(False, (is_f8_normal == None))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "checksum": "c2783b6ea1f1095a87807bc54a17e26c",
     "grade": false,
     "grade_id": "exercise-2-4-3-text",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Question 3 (4 points)\n",
    "\n",
    "Suppose you want to verify whether features <code>**f3**</code> and <code>**f15**</code> suffer from the presence of _outliers_. To do so, you need to visualize the boxplots of both those variables (i.e., using <code>**seaborn.boxplot**</code> method); each plot should be assigned to the two variable below: <code>**box_plot_f3**</code> and <code>**box_plot_f15**</code>. Once plots are ready, you should assign the variable <code>**col_outliers**</code> a list of column labels, corresponding to the names of the columns you believe are containing outliers. As such, <code>**col_outliers**</code> may take on **4** possible values: \n",
    "-  <code>**[]**</code> if both <code>**f3**</code> and <code>**f15**</code> **do not** contain any outlier;\n",
    "-  <code>**['f3']**</code> if **only** <code>**f3**</code> contains any outlier;\n",
    "-  <code>**['f15']**</code> if **only** <code>**f15**</code> contains any outlier;\n",
    "-  <code>**['f3', 'f15']**</code> if both <code>**f3**</code> and <code>**f15**</code> **do** have outliers.\n",
    "\n",
    "Once you have successfully valued those three variables, you are asked to implement the function <code>**log_smooth_outliers**</code> below, which takes as input a <code>**pandas.DataFrame**</code> and a list of column labels that need to be log-transformed. For each <code>**{COL_NAME}**</code> in the list you need to create a **new** column in the dataframe, whose label name is <code>**'log_'+{COL_NAME}**</code> and whose value is the _smoothed_ logarithm of the original value in <code>**{COL_NAME}**</code>. The smoothed logarithm transformation of a value $x$ is computed as $\\log(x + 1)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "checksum": "7e0a4ce183d3bbeddeb945c046339c90",
     "grade": false,
     "grade_id": "exercise-2-4-3-code",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "box_plot_f3 = None\n",
    "box_plot_f15 = None\n",
    "col_outliers = None\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "checksum": "9b36861bcd9053c805c5db76453546d1",
     "grade": false,
     "grade_id": "exercise-2-4-3-code-2",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def log_smooth_outliers(data, col_outliers):\n",
    "    \"\"\"\n",
    "    Takes as input a pandas.DataFrame and a list of column names containing outliers.\n",
    "    For each of those columns, it creates a new column labeled as 'log_'{COL_NAME}\n",
    "    (where {COL_NAME} is the label in the list) and whose values are obtained by applying\n",
    "    a smooth log transformation to the original values of {COL_NAME}.\n",
    "    The smooth log-transformation of a value x is: log(x + 1), so as to avoid any runtime warning\n",
    "    induced by possible log(0) computation.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "checksum": "0d03656fe2ec5b26ee329e852689e4b3",
     "grade": true,
     "grade_id": "exercise-2-4-3-test",
     "locked": true,
     "points": 3,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Test the correctness of the `box_plot_f11`, `box_plot_f15`, `outliers` and `log_smooth_outliers`\n",
    "\"\"\"\n",
    "\n",
    "log_smooth_data = log_smooth_outliers(data, col_outliers)\n",
    "\n",
    "assert_equal(False, (box_plot_f3 == None))\n",
    "assert_equal(False, (box_plot_f15 == None))\n",
    "assert_equal(False, (col_outliers == None))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
